# -*- coding: utf-8 -*-
"""spellcaster.LCX.index.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11Samq-gxS9j6oFy7nfdJVsk0Jheha_Sp
"""

pip install Levenshtein

import zlib
import matplotlib.pyplot as plt
import numpy as np
import re
import Levenshtein

# --- 1. The Analyzer Class (Adapted for Text) ---
class TextComplexityAnalyzer:
    def __init__(self):
        self.history_str = ""
        self.last_sentence = ""
        self.last_k_hist = 0

    def get_complexity(self, text):
        if not text: return 0
        return len(zlib.compress(text.encode('utf-8')))

    def analyze_flow(self, text_content):
        # 1. Cleaning & Parsing
        # Remove tags and split into sentences
        clean_text = re.sub(r'<[^>]*>', '', text_content)
        # Split by periods, newlines, or bullet points, keeping non-empty entries
        sentences = [s.strip() for s in re.split(r'[.\nâ€¢]+', clean_text) if s.strip()]

        data_k_hist = []
        data_volatility = []
        data_synergy = []

        # 2. The Simulation Loop (Sentence by Sentence)
        for i, sentence in enumerate(sentences):
            # A. Update History (Narrative Accumulation)
            self.history_str += sentence + " "
            k_curr = self.get_complexity(self.history_str)

            # Marginal Information (How much NEW info did this sentence add?)
            delta_k = max(1, k_curr - self.last_k_hist)

            # B. Volatility (Edit Distance from last sentence)
            lev_dist = 0
            if self.last_sentence:
                lev_dist = Levenshtein.distance(self.last_sentence, sentence)

            # C. Synergy (Impact per Bit)
            # Volatility / Marginal Cost
            # High Score = Big change in words, but fits the history pattern well.
            synergy = lev_dist / delta_k if delta_k > 0 else 0

            # Store Data
            data_k_hist.append(k_curr)
            data_volatility.append(lev_dist)
            data_synergy.append(synergy)

            # Update State
            self.last_sentence = sentence
            self.last_k_hist = k_curr

        return {
            "sentences": sentences,
            "k_hist": data_k_hist,
            "volatility": data_volatility,
            "synergy": data_synergy
        }

# --- 2. Load the Data (Reading from .txt files) ---

file_path_oi = input("Please enter the path for the first .txt file (e.g., 'gst.gg.doherty.txt'): ")
file_path_gg = input("Please enter the path for the second .txt file (e.g., 'gst.gg.doherty.txt'): ")

try:
    with open(file_path_oi, 'r', encoding='utf-8') as f:
        text_oi = f.read()
    with open(file_path_gg, 'r', encoding='utf-8') as f:
        text_gg = f.read()
except FileNotFoundError:
    print("One or both files not found. Please ensure the paths are correct.")
    text_oi = ""
    text_gg = ""

# --- 3. Execute Analysis ---
analyzer_oi = TextComplexityAnalyzer()
results_oi = analyzer_oi.analyze_flow(text_oi)

analyzer_gg = TextComplexityAnalyzer()
results_gg = analyzer_gg.analyze_flow(text_gg)

# --- 4. Plotting ---
def smooth(data, window=3):
    if len(data) == 0: return np.array([])
    if len(data) < window: return np.array(data)
    return np.convolve(data, np.ones(window)/window, mode='valid')

# Set dark background style
plt.style.use('dark_background')

fig, axes = plt.subplots(3, 1, figsize=(12, 12))

# Plot 1: Narrative Accumulation (History Complexity)
ax1 = axes[0]
ax1.plot(results_oi['k_hist'], label='Human Control', color='#00FFFF') # Cyan
ax1.plot(results_gg['k_hist'], label='Final 25% GPT 5.2', color='#FF00FF', linestyle='--') # Magenta
ax1.set_title('1. Language Complexity Accumulation (LZ77 History)\n(Steeper slope = More unique info introduced per sentence)', color='white')
ax1.set_ylabel('Compressed Size (Bytes)', color='white')
ax1.tick_params(axis='x', colors='white')
ax1.tick_params(axis='y', colors='white')
ax1.legend(labelcolor='white')
ax1.grid(True, alpha=0.3, color='gray')

# Plot 2: Lexical Volatility (Levenshtein)
ax2 = axes[1]
ax2.plot(results_oi['volatility'], label='Human Control', color='#00FF00', alpha=0.8) # Green
ax2.plot(results_gg['volatility'], label='Final 25% GPT 5.2', color='#FFA500', alpha=0.8, linestyle='--') # Orange
ax2.set_title('2. Language Encoding Volatility (Levenshtein)\n(Higher = More distinct change between sentences)', color='white')
ax2.set_ylabel('Edit Distance', color='white')
ax2.tick_params(axis='x', colors='white')
ax2.tick_params(axis='y', colors='white')
ax2.legend(labelcolor='white')
ax2.grid(True, alpha=0.3, color='gray')

# Plot 3: Cognitive Synergy (Trend)
ax3 = axes[2]
# Smooth the synergy to see the "Arc of Thought"
syn_oi_smooth = smooth(results_oi['synergy'], window=4)
syn_gg_smooth = smooth(results_gg['synergy'], window=4)

ax3.plot(syn_oi_smooth, label='Human Control', color='#FFFF00', linewidth=2) # Yellow
ax3.plot(syn_gg_smooth, label='Final 25% GPT 5.2', color='#FF69B4', linewidth=2, linestyle='--') # Hot Pink
ax3.set_title('3. LBX | Language Behavior Index\n(Efficiency of Complexity Constraint)', color='white')
ax3.set_ylabel('Synergy Ratio', color='white')
ax3.set_xlabel('Sentence Index (Time)', color='white')
ax3.tick_params(axis='x', colors='white')
ax3.tick_params(axis='y', colors='white')
ax3.legend(labelcolor='white')
ax3.grid(True, alpha=0.3, color='gray')

plt.tight_layout()
plt.show()